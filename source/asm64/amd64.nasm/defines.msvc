;  The MIT License (MIT)
; 
; Copyright (c) 2015 mehdi sotoodeh
; 
; Permission is hereby granted, free of charge, to any person obtaining 
; a copy of this software and associated documentation files (the 
; "Software"), to deal in the Software without restriction, including 
; without limitation the rights to use, copy, modify, merge, publish, 
; distribute, sublicense, and/or sell copies of the Software, and to 
; permit persons to whom the Software is furnished to do so, subject to 
; the following conditions:
; 
; The above copyright notice and this permission notice shall be included 
; in all copies or substantial portions of the Software.
; 
; THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS 
; OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF 
; MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. 
; IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY 
; CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, 
; TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE 
; SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
;

;
; Definitions for NASM assembler
; ABI: MSVC
;
; Tested with NASM version 2.11.08

%macro  PUBPROC 1
section .text
    global %1
%1:
%endmacro

%macro  ENDPROC 1
%endmacro

; Restore these registers across calls: rsp, rbx, rbp, rdi, rsi, r12,r13,r14,r15

; 128-bit accumulator
%define ACL     rax
%define ACH     rdx 
%define CARRY   ACH

; 256-bit accumulator-A
; Accumulator-A uses volatile registers
%define A0      r8
%define A1      r9
%define A2      r10
%define A3      r11

; 256-bit accumulator-B
%define B0      r12
%define B1      r13
%define B2      r14
%define B3      r15

%define C0      rcx
%define C1      rbx
%define C2      rdi
%define C3      rsi
%define C4      rbp

; From left to right
%define ARG1    rcx
%define ARG2    rdx
%define ARG3    r8
%define ARG4    r9

; ARG switching 
%define ARG1M   rdi
%define ARG2M   rsi
%define ARG3M   rbx
%define ARG4M   rbp

%macro SaveArg1 0
    push    ARG1M
    mov     ARG1M,ARG1
%endmacro

%macro RestoreArg1 0
    pop     ARG1M
%endmacro

%macro SaveArg2 0
    push    ARG2M
    mov     ARG2M,ARG2
%endmacro

%macro RestoreArg2 0
    pop     ARG2M
%endmacro

%macro SaveArg3 0
    push    ARG3M
    mov     ARG3M,ARG3
%endmacro

%macro RestoreArg3 0
    pop     ARG3M
%endmacro

%macro SaveArg4 0
    push    ARG4M
    mov     ARG4M,ARG4
%endmacro

%macro RestoreArg4 0
    pop     ARG4M
%endmacro

%macro PushB 0
    push    B3
    push    B2
    push    B1
    push    B0
%endmacro

%macro PopB 0
    pop     B0
    pop     B1
    pop     B2
    pop     B3
%endmacro

%macro LOADA 1
    mov     A3,[%1+24]
    mov     A2,[%1+16]
    mov     A1,[%1+8]
    mov     A0,[%1]
%endmacro

%macro STOREA 1
    mov     [%1+24],A3
    mov     [%1+16],A2
    mov     [%1+8],A1
    mov     [%1],A0
%endmacro

%macro LOADB 1
    mov     B3,[%1+24]
    mov     B2,[%1+16]
    mov     B1,[%1+8]
    mov     B0,[%1]
%endmacro

%macro STOREB 1
    mov     [%1+24],B3
    mov     [%1+16],B2
    mov     [%1+8],B1
    mov     [%1],B0
%endmacro

; _______________________________________________________________________
;
; Out:  Y += X
; _______________________________________________________________________

%macro ADD4 8
    add     %4, %8
    adc     %3, %7
    adc     %2, %6
    adc     %1, %5
%endmacro

%macro ADDA 4
    ADD4 A3,A2,A1,A0, %1,%2,%3,%4
%endmacro

%macro ADDB 4
    ADD4 B3,B2,B1,B0, %1,%2,%3,%4
%endmacro

; _______________________________________________________________________
;
; Out:  Y -= X
; _______________________________________________________________________

%macro SUB4 8
    sub     %4, %8
    sbb     %3, %7
    sbb     %2, %6
    sbb     %1, %5
%endmacro
    
%macro SUBA 4
    SUB4 A3,A2,A1,A0, %1,%2,%3,%4
%endmacro

%macro SUBB 4
    SUB4 B3,B2,B1,B0, %1,%2,%3,%4
%endmacro

; _______________________________________________________________________
; MULT(a,b)
; IN:   a, b
; Out:  ACH:ACL = a*b
; _______________________________________________________________________
%macro MULT 2
    mov     ACL,%1
    mul     qword %2
%endmacro
   
; _______________________________________________________________________
; MULSET(u,v,a,b)
; IN:   a, b
; Out:  u:v = a*b
; _______________________________________________________________________
%macro MULSET 4
    MULT    %3,%4
    mov     %2,ACL
    mov     %1,ACH
%endmacro
   
; _______________________________________________________________________
; MULADD (u,v,a,b)
; IN:   a, b
; Out:  u:v += a*b
; _______________________________________________________________________
%macro MULADD 4
    MULT    %3,%4
    add     %2,ACL
    adc     %1,ACH
%endmacro

; _______________________________________________________________________
;
; IN:   a
; Out:  ACH:ACL = a*a
; _______________________________________________________________________
%macro SQR  1
    mov     ACL,%1
    mul     ACL
%endmacro

; _______________________________________________________________________
; SQRSET(u,v,a)
; IN:   a
; Out:  u:v = a*a
; _______________________________________________________________________
%macro SQRSET 3
    SQR     %3
    mov     %2,ACL
    mov     %1,ACH
%endmacro

; _______________________________________________________________________
; SQRADD(u,v,a)
; IN:   a
; Out:  u:v += a*a
; _______________________________________________________________________
%macro SQRADD 3
    SQR     %3
    add     %2,ACL
    adc     %1,ACH
%endmacro

; _______________________________________________________________________
; MULADD_W0(ZZ,YY,BB,XX)
; Out:  CARRY:Z = b*X + Y
; _______________________________________________________________________

%macro MULADD_W0 4 ; (ZZ,YY,BB,XX)
    MULT    %4,%3
    add     ACL,%2
    adc     ACH,0
    mov     %1,ACL
%endmacro

; _______________________________________________________________________
; MULADD_W1(ZZ,YY,BB,XX)
; Out: CARRY:Z = b*X + Y + CARRY
;      ZF = set if no carry
; _______________________________________________________________________

%macro MULADD_W1 4
    mov     %1,ACH
    MULT    %4,%3
    add     ACL,%2
    adc     ACH,0
    add     %1,ACL
    adc     ACH,0
%endmacro

; _______________________________________________________________________
; MULSET_W0(YY,BB,XX)
; Out:  CARRY:Y = b*X
; _______________________________________________________________________

%macro MULSET_W0 3
    MULT    %3,%2
    mov     %1,ACL
%endmacro

; _______________________________________________________________________
; MULSET_W1(YY,BB,XX)
; Out: CARRY:Y = b*X + CARRY
;      ZF = set if no carry
; _______________________________________________________________________

%macro MULSET_W1 3
    mov     %1,ACH
    MULT    %3,%2
    add     %1,ACL
    adc     ACH,0
%endmacro

; _______________________________________________________________________
; RL_MSBS(AA,XX)
; Out: AA <<== upper bits of XX.hi, XX.lo
;      XX <<= 1
; _______________________________________________________________________
%macro RL_MSBS 2
    shl     %2,1
    rcl     %1,1
    bt      %2,32
    rcl     %1,1
%endmacro
